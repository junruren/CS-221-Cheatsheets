\newcommand{\agentcolor}[1]{\textcolor{Mahogany}{#1}}
\newcommand{\oppcolor}[1]{\textcolor{MidnightBlue}{#1}}
\newcommand{\chancecolor}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\dcolor}[1]{\textcolor{Fuchsia}{#1}}

\subsection{Games}

\Blue{Game tree} describes the possibilities of a game and models opponents \&
randomness. Each node is a decision point for a player; each root-to-leaf path
is a possible outcome of the game.  Legend: \agentcolor{$\bigtriangleup$ -
maximizing node}, \oppcolor{$\bigtriangledown$ - minimizing node}, and
\chancecolor{$\bigcirc$ - chance node}

\Blue{Two-player zero-sum game} Each state is fully observed and such that
players take turns; utility of the agent is negative the utility of the opponent
(so the sum of the two utilities is zero).
\begin{itemize}
    \item $\text{Players}$: $ = \{\agentcolor{\text{agent}}, \oppcolor{\text{opp}}\}$
    \item $s_{start}$, $\text{Actions}(s)$, $\text{Succ}(s, a)$, $\text{IsEnd}(s)$
    \item $\text{Utility}(s)$: agent's utility for end state $s$
    \item $\text{Player}(s)$: player who controls the state $s$
\end{itemize}

\Green{Types of policies}\\
\textbf{Stochastic policies}: $\pi_{p}(s, a) \in \left[0, 1\right]$
\underline{probability} of player $p$ taking action $a$ in state $s$.\\
\textbf{Deterministic policies}: $\pi_{p}(s) \in \text{Actions}(s)$ action that
player $p$ takes in state $s$. A (special) instance of Stochastic policies.\\

\Blue{Game evaluation} analogous to recurrence for policy evaluation in MDPs.
$V_{\text{eval}}(s) = \begin{cases}
    \text{Utility}(s) & \text{IsEnd}(s) \\
    \sum_{a \in \text{A}(s)} \pi_{\text{ag}}(s, a) V_{\text{eval}}(\text{Suc}(s, a)) & \text{Playr}(s) = \text{ag} \\
    \sum_{a \in \text{A}(s)} \pi_{\text{op}}(s, a) V_{\text{eval}}(\text{Suc}(s, a)) & \text{Playr}(s) = \text{op} \\
\end{cases}$

As the agent, we want to solve \agentcolor{$\pi_{\text{agent}}(s, a)$}: the best
thing we should do.

\Green{Expectimax} $V_{\text{exptmax}}(s)$ is the max expected utility of any
agent policy when playing w.r.t. a \emph{fixed and known}
\oppcolor{$\pi_{\text{opp}}$}.
$V_{\text{exptmax}}(s) = \begin{cases}
    \text{Utility}(s) & \text{IsEnd}(s) \\
    \agentcolor{\max_{a \in \text{A}(s)}} V_{\text{e-m}}(\text{Suc}(s, a)) & \text{Playr}(s) = \text{ag} \\
    \sum_{a \in \text{A}(s)} \pi_{\text{op}}(s, a) V_{\text{e-m}}(\text{Suc}(s, a)) & \text{Playr}(s) = \text{op} \\
\end{cases}$ \\
$\Rightarrow \underline{\agentcolor{\pi_{\text{exptmax}(7)}},
\oppcolor{\pi_{\text{7}}}}$ (assuming the fixed opponent policy
$\oppcolor{\pi_{\text{opp}}}$ is $\oppcolor{\pi_{\text{7}}}$, then the the best
policy computed by expectimax recurrence for agent is denoted as
$\agentcolor{\pi_{\text{exptmax}(7)}}$).

\Blue{Minimax} Find an optimal agent policy against an adversary by assuming the
worst case: the opponent does everything to minimize the agent's utility.
$V_{\text{minimax}}(s) = \begin{cases}
    \text{Utility}(s) & \text{IsEnd}(s) \\
    \agentcolor{\max_{a \in \text{A}(s)}} V_{\text{m-m}}(\text{Suc}(s, a)) & \text{Playr}(s) = \text{ag} \\
    \oppcolor{\min_{a \in \text{A}(s)}} V_{\text{m-m}}(\text{Suc}(s, a)) & \text{Playr}(s) = \text{op} \\
\end{cases}$\\
$\Rightarrow \underline{\agentcolor{\pi_{\text{max}}}, \oppcolor{\pi_{\text{min}}}}$:
$\agentcolor{\pi_{\text{max}}(s)} = \arg \max_{a \in \text{A}(s)} V_{\text{minimax}}(\text{Suc}(s, a))$\\
$\oppcolor{\pi_{\text{min}}(s)} = \arg \min_{a \in \text{A}(s)} V_{\text{minimax}}(\text{Suc}(s, a))$\\

\Red{Minimax properties} we can play an agent policy
$\agentcolor{\pi_{\text{agent}}}$ against an opponent policy
$\oppcolor{\pi_{\text{opp}}}$, which produces an expected utility via game
evaluation, denoted as $V(\agentcolor{\pi_{\text{agent}}},
\oppcolor{\pi_{\text{opp}}})$\\
\begin{enumerate}
    \item if the agent were to change its policy from $\pi_{\text{max}}$ to any
        $\pi_{\text{agent}}$, then the agent wouldn't be better off (and in
        general, worse off). \fbox{$\forall \agentcolor{\pi_{\text{agent}}},
        V(\agentcolor{\pi_{\text{max}}}, \oppcolor{\pi_{\text{min}}}) \ge
        V(\agentcolor{\pi_{\text{agent}}}, \oppcolor{\pi_{\text{min}}})$}
    \item if the opponent were to change its policy from $\pi_{\text{min}}$ to
        any $\pi_{\text{opp}}$, then the opponent wouldn't be better off (the
        value of the game can only increase, which is favorable to the agent).
        \fbox{$\forall \oppcolor{\pi_{\text{opp}}},
        V(\agentcolor{\pi_{\text{max}}}, \oppcolor{\pi_{\text{min}}}) \le
        V(\agentcolor{\pi_{\text{max}}}, \oppcolor{\pi_{\text{opp}}})$}
        From the agent's point of view, this can be interpreted as guarding
        against the worst case $\Rightarrow$ If $V_{\text{minimax}}(s) = 1$, the
        agent is guaranteed at least a value of 1 no matter what the opponent
        does.
    \item if the opponent is known to be not adversarial, then the minimax
        policy might not be optimal for the agent.
        \fbox{For $\oppcolor{\pi_{\text{7}}}$, $V(\agentcolor{\pi_{\text{max}}},
        \oppcolor{\pi_{\text{7}}}) \le V(\agentcolor{\pi_{\text{exptmax}(7)}},
        \oppcolor{\pi_{\text{7}}})$}
\end{enumerate}
\begin{displaymath}
\boxed{
\begin{split}
V(\agentcolor{\pi_{\text{exptmax}(7)}}, \oppcolor{\pi_{\text{min}}}) &\le V(\agentcolor{\pi_{\text{max}}}, \oppcolor{\pi_{\text{min}}}) \\
\le V(\agentcolor{\pi_{\text{max}}}, \oppcolor{\pi_{\text{opp}}}) & \le V(\agentcolor{\pi_{\text{exptmax}(7)}}, \oppcolor{\pi_{\text{7}}})
\end{split}
}
\end{displaymath}

\Blue{Expectiminimax} \textbf{$\text{Players}$}: $ =
\{\agentcolor{\text{agent}}, \oppcolor{\text{opp}},
\chancecolor{\text{coin}}\}$: a third player representing any sort of natural
randomness (metaphorically ``coin'') is introduced which always follows a known
stochastic policy.
$V_{\text{exptminmax}}(s) = \begin{cases}
    \text{Utility}(s) & \text{IsEnd}(s) \\
    \agentcolor{\max_{a \in \text{A}(s)}} V_{\text{e-m-m}}(\text{Suc}(s, a)) & \text{Playr}(s) = \agentcolor{\text{ag}} \\
    \oppcolor{\min_{a \in \text{A}(s)}} V_{\text{e-m-m}}(\text{Suc}(s, a)) & \text{Playr}(s) = \oppcolor{\text{op}} \\
    \chancecolor{\sum_{a \in \text{A}(s)} \pi_{\text{co}(s,a)}} V_{\text{e-m-m}}(\text{Suc}(s, a)) & \text{Playr}(s) = \chancecolor{\text{co}}
\end{cases}$

\subsubsection{Speeding up minimax}

\Green{Depth-limited tree search} Stop at maximum depth $d_{\text{max}}$. Use:
at state $s$, call $V_{\text{minmax}}(s, d_{\text{max}})$. Convention: decrement
depth at last player's turn.
$V_{\text{minmax}}(s, \dcolor{d}) = \begin{cases}
    \text{Utility}(s) & \text{IsEnd}(s) \\
    \dcolor{\text{Eval}(s)} & \dcolor{d = 0}\\
    \agentcolor{\max_{a \in \text{A}(s)}} V_{\text{e-m-m}}(\text{Suc}(s, a), \dcolor{d}) & \text{Playr}(s) = \agentcolor{\text{ag}} \\
    \oppcolor{\min_{a \in \text{A}(s)}} V_{\text{e-m-m}}(\text{Suc}(s, a), \dcolor{d-1}) & \text{Playr}(s) = \oppcolor{\text{op}} \\
\end{cases}$

\Green{Evaluation function} a domain-specific and possibly very weak estimate of
the value $V_{\text{minmax}}(s)$, analogous to $A^\star$'s
$\text{FutureCost}(s)$ but unlike $A^\star$ no guarantees on the error from
approximation.

\Green{Depth-limited exhaustive search} $O(b^{2d})$ time. Still not ideal.

\Red{Optimal path} path that minimax policies take. Values of all the nodes on
path are the same.

\Blue{Alpha-beta pruning} a domain-general exact method optimizing the minimax
algorithm by avoiding the unnecessary exploration of parts of the game tree. To
do so, each player keeps track of the best value they can hope for (stored in
$\alpha$ for the maximizing player and in $\beta$ for the minimizing player). At
a given step, $\beta < \alpha \Rightarrow$ the optimal path is \underline{not}
going to be in the current branch as the earlier player had a better option at
their disposal.

Order matters:
\begin{itemize}
    \item Worst ordering: $O(b^{2d})$ time
    \item Best ordering: $O(b^{2\cdot0.5d})$ time
    \item Random ordering: $O(b^{2\cdot0.75d})$ time when $b = 2$
\end{itemize}
In practice, can use $\text{Eval}(s)$:\begin{itemize}
    \item on a max node, order successors by decreasing $\text{Eval}(s')$
    \item on a min node, order successors by increasing $\text{Eval}(s')$
\end{itemize}

\Blue{Temporal difference (TD) learning} picks a piece of experience
$(s,a,r,s')$ and updates $\mathbf{w}$. Used when we don't know the transitions /
rewards. The value is based on exploration policy.\\
Evaluation function could be hand-crafted but also learned from data:
$\text{Eval}(s) = V(s;\mathbf{w}) = \mathbf{w}\cdot\phi(s)$ (linear).\\
\begin{displaymath}
\begin{split}
    &\mathbf{w} \leftarrow \mathbf{w} - \eta \\
    &\left[\textcolor{red}{\underbrace{\hat{V_\pi}(s;\mathbf{w})}_{\text{prediction}}}
    - \textcolor{Green}{\underbrace{(r + \gamma
    \hat{V_\pi}(s';\mathbf{w}))}_{\text{target}}}\right]
    \textcolor{blue}{\nabla_{\mathbf{w}}\hat{V_\pi}(s;\mathbf{w})}
\end{split}
\end{displaymath}
For linear functions: $\textcolor{red}{V(s;\mathbf{w})} = \mathbf{w} \cdot
\phi(s)$; $\textcolor{blue}{\nabla_{\mathbf{w}}V(s;\mathbf{w})} = \phi(s)$.
\fbox{
    $\mathbf{w} \leftarrow \mathbf{w} -
    \eta\left[\textcolor{red}{\mathbf{w} \cdot \phi(s)} - \textcolor{Green}{(r + \gamma
    \mathbf{w} \cdot \phi(s'))}\right]\textcolor{blue}{\phi(s)}$
}\\
Feature selection: how good my ``board'' is.

\Blue{TD learning vs Q-learning} \textbf{Q-learning} operates on
$\hat{Q}_{\text{opt}}(s,a;\mathbf{w})$, \underline{off-policy} (value based on
estimate of optimal policy), and doesn't need to know MDP transitions
$T(s,a,s')$. \textbf{TD learning}: operates on $\hat{V_\pi}(s;\mathbf{w})$,
\underline{on-policy} (value is based on exploration policy), and \textbf{needs
to know rules of the game $\text{Succ}(s,a)$}.

\subsubsection{Simultaneous games}

On the contrary of turn-based games, no ordering on the player's moves in
simultaneous games.

\Green{Single-move simultaneous game} \textbf{Players} $= \left\{A, B\right\}$
with given possible actions. \textbf{$V(a,b)$}: \textbf{\underline{A's} utility}
if $A$ chooses action $a$ and $B$ chooses action $b$. \textbf{Payoff matrix}: $V
\in |\text{Actions}|^2$.

\Red{Pure strategy} just a single action: \fbox{$a \in \text{Actions}$}.
\Red{Mixed strategy} a probability distribution over actions:
\fbox{$\forall a \in \text{Actions}, 0 \le \pi(a) \le 1$}. Examples:
\begin{itemize}
    \item Fixed, always show ``1'': $\pi = [1,0]$
    \item Fixed, always show ``2'': $\pi = [0,1]$
    \item Mixed, uniformly random: $\pi = [\frac{1}{2}, \frac{1}{2}]$
\end{itemize}

\Blue{Game evaluation} the \underline{value} of the game if player $A$ follows
$\pi_A$ and player $B$ follows $\pi_B$:
\fbox{$V(\pi_A, \pi_B) = \sum_{a,b}\pi_A(a)\pi_B(b)V(a,b)$}

\Red{von Neumann minimax theorem} for every simultaneous two-player zero-sum
game with a finite number of actions:
\fbox{$\agentcolor{\max_{\pi_A}} \oppcolor{\min_{\pi_B}} V(\pi_A, \pi_B) =
\oppcolor{\min_{\pi_B}} \agentcolor{\max_{\pi_A}} V(\pi_A, \pi_B)$} where
$\pi_A$, $\pi_B$ range over \textbf{mixed strategies}.

\subsubsection{Non-zero games}

\Blue{Payoff matrix} utility for player $p$: $V_p(\pi_A, \pi_B)$.\\
\Red{Nash equilibrium} $(\pi_A^\star, \pi_B^\star)$ such that no player has an
incentive to change their strategy:
\fbox{$\forall \pi_A, V_A(\pi_A^\star, \pi_B^\star) \ge V_A(\pi_A, \pi_B^\star)$} and
\fbox{$\forall \pi_B, V_B(\pi_A^\star, \pi_B^\star) \ge V_B(\pi_A^\star, \pi_B)$}.
\Blue{Nash's existence theorem} in any finite-player game with finite number of
actions, there exists \textbf{at least one} Nash equilibrium.