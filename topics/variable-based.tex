\subsection{Constraint satisfaction problems}

\Blue{Factor graph} (aka Markov random field) a set of \underline{variables} $X
= {X_1,\dots,X_n}$ where $X_i \in \text{Domain}_i$ and \underline{factors}
$f_1,\dots,f_m$, with each $f_j(X) \ge 0$.
\Hint{Each factor is implemented as checking a solution rather than computing
the solution.}

\Green{Domain} possible values to be assigned to a variable.

\Red{Scope of a factor $f_j$} the set of variables $f_j$ depends on.
\Red{Arity} the size of this set. ``Unary factors'' (arity 1); ``Binary
factors'' (arity 2). ``Constraints'' (factors that return 0 or 1).

\Blue{Assignment weight} each \underline{assignment $x = (x_1, \dots, x_n)$}
yields a $\text{Weight}(x)$ defined as being the product of all factors $f_j$
applied to that assignment.
\fbox{$\text{Weight}(x) = \Pi_{j=q}^{m} f_j(x)$} ($x$ in its entirety is passed
in to each $f_j$ for simplicity of this notation, though in reality only a
subset of $x$ would be needed for $f_j$)

\Green{CSP} a factor graph where all factors are binary.
\fbox{For $j = 1,\dotsm$, $f_j(x) \in \{0,1\}$} (the constraint $j$ with
assignment $x$ is said to be satisfied iff $f_j(x) = 1$.)

\Green{Consistent assignment $x$ of a CSP} iff $\text{Weight}(x) = 1$ (i.e., all
constrains are satisfied.)

\Green{Dependent factors $D(x,X_i)$} a set of factors depending on $X_i$ but not
on unassigned variables.

\Red{Backtracking search} find maximum weight assignment of a factor graph.
\textbf{$\text{Backtrack}(x, w, \text{Domains})$}
\begin{enumerate}
    \item choose an unassigned \textbf{variable} $X_i$ \Hint{(MCV)}
    \item order \textbf{values} of $X_i$'s Domain \Hint{(LCV)}
    \item for each value $v$ in the order:\begin{enumerate}
        \item $\delta \leftarrow \Pi_{f_i\in D(x,X_i)}f_j(x\cup\left\{X_i:v\right\})$
        \item if $\delta = 0$: continue
        \item $\text{Domains}' \leftarrow \text{Domains}$ via \textbf{lookahead} \Hint{(forward checking)}
        \item if $\text{Domains}_i'$ is empty: continue
        \item \textbf{$\text{Backtrack}(x \cup \left\{X_i:v\right\}, w\delta, \text{Domains}')$}
    \end{enumerate}
\end{enumerate}
\begin{itemize}
    \item Strategy: extends partial assignments
    \item Optimality: exact
    \item Time: exponential
\end{itemize}

\Blue{Forward checking} one-step lookahead heuristic that preemptively removes
inconsistent values from the domains of neighboring variables.
\begin{itemize}
    \item After assigning a variable $X_i$, it eliminates inconsistent values from the domains of all its neighbors.
    \item If any of these domains become empty, stop the local backtracking search.
    \item if we unassign a variable $X_i$, have to restore the domain of its neighbors.
\end{itemize}

\Green{Most constrained \underline{variable}} selects the next unassigned
variable that has the fewest consistent values: fail early, prune early.

\Green{Least constrained \underline{value}} assigns the next value that yields
the highest number of consistent values of neighboring variables: prefers the
value that is most likely to work.

\Green{Arc consistency of variable $X_i$} w.r.t. $X_j$ is enforced when for each
$x_i \in \text{Domain}_i$, there exists $x_j \in \text{Domain}_j$ such that any
factos between $X_i$ and $X_j$ is non-zero.

\Red{AC-3} a multi-step lookahead heuristic that applies forward checking to all
relevant variables. After a given assignment, it performs forward checking and
then successively enforces arc consistency w.r.t. the neighbors of variables for
which the domain change during the process.
\Hint{AC-3 only looks locally at the graph for nothing blatantly wrong; it can't
detect when there are no consistent assignments.}

\Red{Beam search} extends partial assignments of $n$ variables of branching
factor $b = |\text{Domain}|$ by exploring the $K$ top paths at each step. The
beam size $1 \ge K \ge b^n$ controls the tradeoff between efficiency and
accuracy.
\Hint{Runtime is Linear to $n$: $O(n \underbrace{Kb
\log(Kb)}_{\text{sorting top K}})$}. $K=1$: greedy search ($O(nb)$ time); $K
\rightarrow +\infty$: BFS ($O(b^n)$ time).
\begin{itemize}
    \item Strategy: extends partial assignments
    \item Optimality: approximate
    \item Time: linear
\end{itemize}

\Blue{Local search (iterated conditional modes)} modifies the assignment of a
factor graph one variable at a time until convergence. AT step $i$, assign to
$X_i$ the value $v$ that maximizes the product of all factors connected to that
variable. \Hint{ICM may get stuck in local optima; adding randomness may help.}
\begin{itemize}
    \item Strategy: modify complete assignments
    \item Optimality: approximate
    \item Time: linear
\end{itemize}

\subsection{Markov Networks}

\begin{tabular}{|c|c|c|c|} 
    \hline
    \textbf{CSPs} & \textbf{Markov networks} \\
    \hline
    variables & random variables \\ 
    \hline
    weights & probabilities \\
    \hline
    max weight assignment & marginal probabilities \\
    \hline
\end{tabular}

Capture the uncertainty over assignment using the language of probability.

\Green{Markov Network} a factor graph which defines a joint distribution over
random variables $X = (X_1,\dots,X_n)$:
\fbox{$\mathbb{P}(X=x)=\frac{\text{Weight}(x)}{Z}$}

\Green{$Z = \sum_{x'} \text{Weight}(x')$} sum all the possible assignments'
weights (normalization constant)

\Green{Marginal probability} the probability of when one particular variable
$X_i$ is assigned with a particular value $v$: sum $\mathbb{P}$ when $X_i = v$
\fbox{$\mathbb{P}(X_i=v)=\sum_{x:x_i=v}\mathbb{P}(X=x)$}

\Blue{Gibbs sampling} Initialize $x$ to a random complete assignment.
Loop through $i=1,\dots,n$ until convergence: \begin{itemize}
    \item Set $x_i = v$ with probability $\mathbb{P}(X_i = v | \underbrace{X_{-i}}_{\text{all vars except} X_i}=x_{-i})$
    \item Increment $\text{count}_i(x_i)$ (how often this assignment is encountered. \Hint{can just track particular vars we're interested in.})
\end{itemize}
Estimate $\hat{\mathbb{P}}(X_i=x_i)=\frac{\text{count}_i(x_i)}{\sum_{v}\text{count}_i(v)}$

\begin{tabular}{|c|c|c|c|} 
    \hline
    \textbf{ICM} & \textbf{Gibbs sampling} \\
    \hline
    max weight & marginal probabilities \\ 
    assignment &   \\ 
    \hline
    choose best value & sample a value \\
    \hline
    converges to & marginals converge to\\
    local optimum & correct answer \\
    \hline
\end{tabular}

\subsection{Bayesian Networks}

\Blue{Explaining away} suppose two causes positively influence an effect.
Conditioned on the effect, further conditioning on one causes reduces the
probability of the other cause.

\Red{Bayesian network} a durected acyclic graph that specifies a joint distribution
over random variables $X=(X_1,\dots,X_n)$ as a product of local conditional distributions,
one for each node: \fbox{$\mathbb{{P}}(X_1=x_1,\dots,X_n=x_n) = \prod_{i=1}^{n}
p(x_i|x_{\text{Parents}(i)})$}

\Blue{Probabilistic program} randomizes variable assignment such that we can
write down complex Bayesian networks that generates assignments without having
to explicitly specify associated probabilities.
\Hint{Unlike normal classification (e.g., neural nets), Bayesian networks
provide a different paradigm where we think about going from output to the
input.}

\Green{Probabilistic inference strategy} to compute the probability $P(Q|E=e)$
of query $Q$ given evidence $E=e$:
\begin{enumerate}
    \item Remove vars that aren't ancestors of the query $Q$ or the evidence $E$ by marginalization
    \item Convert Bayesian network to factor graph
    \item Condition on the evidence $E=e$
    \item Remove nodes disconnected from the query $Q$ by marginalization
    \item Run probabilistic inference algorithm
\end{enumerate}
\Blue{Filtering question} asks for the distribution of some hidden variable
$H_i$ conditioned on only the evidence up until that point. \Hint{Useful for
real-time object tracking as the future can't be seen.}
\Blue{Smoothing question} asks for the distribution of some hidden variable
$H_i$ conditioned on on the evidence including the future. \Hint{Useful when all
the data have been collected and we want to retrospectively go and figure out
what the hidden state $H_i$ was.}

\Red{Forward-backward algorithm} computes the exact value of $P(H=h_k|E=e)$ a
smoothing query) for any $k\in\left\{1,\dots,L\right\}$ in the case of an HHM of
size $L$.
\begin{enumerate}
    \item for $i\in \left\{1,\dots,L\right\}$, compute $F_i(h_i)=\sum_{h_{i-1}}F_{i-1}(h_{i-1})p(h_i|h_{i-1})p(e_i|h_i)$
    \item for $i\in \left\{L,\dots,1\right\}$, compute $B_i(h_i)=\sum_{h_{i+1}}B_{i+1}(h_{i+1})p(h_{i+1}|h_i)p(e_{i+1}|h_{i+1})$
    \item for $i\in \left\{1,\dots,L\right\}$, compute $S_i(h_i)=\frac{F_i(h_i)B_i(h_i)}{\sum_{h_i}F_i(h_i)B_i(h_i)}$
\end{enumerate}
